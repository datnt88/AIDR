----------------------------------------------------------------------
INFORMATION: dropout_ratio=0.5 minibatch-size=128 filter-nb=150 filt_len=2 pool_len=3 vocab=90
----------------------------------------------------------------------
Loading data...
Nb of tweets: train: 7924 test: 2261 dev: 1129
Total vocabulary size: 13537
Pruned vocabulary size: 90.0% =12183
Loading pre-trained word2vec embeddings......
 Number of words found in emb matrix: 8175 of 12183
............................
7924 train tweets
2261 test  tweets
1129 dev   tweets
12183 vocabulary size
6 different classes
............................
Building model: convolutional neural network (cnn)
Doing classification with class # 6
Training and validating ....
Train on 7924 samples, validate on 1129 samples
Epoch 1/25
Epoch 00000: val_loss improved from inf to 1.25885, saving model to IN_models/cnn-adadelta-150-True-categorical_crossentropy-128-0.5-init-pretrained-90.0-128-128.model.cl.6in
8s - loss: 1.4234 - acc: 0.5524 - val_loss: 1.2588 - val_acc: 0.5926
Epoch 2/25
Epoch 00001: val_loss improved from 1.25885 to 1.17007, saving model to IN_models/cnn-adadelta-150-True-categorical_crossentropy-128-0.5-init-pretrained-90.0-128-128.model.cl.6in
8s - loss: 1.2508 - acc: 0.5920 - val_loss: 1.1701 - val_acc: 0.5979
Epoch 3/25
Epoch 00002: val_loss improved from 1.17007 to 1.11236, saving model to IN_models/cnn-adadelta-150-True-categorical_crossentropy-128-0.5-init-pretrained-90.0-128-128.model.cl.6in
8s - loss: 1.1821 - acc: 0.6013 - val_loss: 1.1124 - val_acc: 0.6182
Epoch 4/25
Epoch 00003: val_loss improved from 1.11236 to 1.06968, saving model to IN_models/cnn-adadelta-150-True-categorical_crossentropy-128-0.5-init-pretrained-90.0-128-128.model.cl.6in
8s - loss: 1.1254 - acc: 0.6138 - val_loss: 1.0697 - val_acc: 0.6413
Epoch 5/25
Epoch 00004: val_loss improved from 1.06968 to 1.04842, saving model to IN_models/cnn-adadelta-150-True-categorical_crossentropy-128-0.5-init-pretrained-90.0-128-128.model.cl.6in
8s - loss: 1.0913 - acc: 0.6219 - val_loss: 1.0484 - val_acc: 0.6413
Epoch 6/25
Epoch 00005: val_loss improved from 1.04842 to 1.02057, saving model to IN_models/cnn-adadelta-150-True-categorical_crossentropy-128-0.5-init-pretrained-90.0-128-128.model.cl.6in
8s - loss: 1.0638 - acc: 0.6292 - val_loss: 1.0206 - val_acc: 0.6492
Epoch 7/25
Epoch 00006: val_loss improved from 1.02057 to 1.00064, saving model to IN_models/cnn-adadelta-150-True-categorical_crossentropy-128-0.5-init-pretrained-90.0-128-128.model.cl.6in
8s - loss: 1.0359 - acc: 0.6383 - val_loss: 1.0006 - val_acc: 0.6510
Epoch 8/25
Epoch 00007: val_loss improved from 1.00064 to 0.99589, saving model to IN_models/cnn-adadelta-150-True-categorical_crossentropy-128-0.5-init-pretrained-90.0-128-128.model.cl.6in
8s - loss: 1.0176 - acc: 0.6444 - val_loss: 0.9959 - val_acc: 0.6484
Epoch 9/25
Epoch 00008: val_loss improved from 0.99589 to 0.96964, saving model to IN_models/cnn-adadelta-150-True-categorical_crossentropy-128-0.5-init-pretrained-90.0-128-128.model.cl.6in
8s - loss: 0.9986 - acc: 0.6441 - val_loss: 0.9696 - val_acc: 0.6554
Epoch 10/25
Epoch 00009: val_loss did not improve
8s - loss: 0.9811 - acc: 0.6518 - val_loss: 0.9932 - val_acc: 0.6475
Epoch 11/25
Epoch 00010: val_loss did not improve
8s - loss: 0.9748 - acc: 0.6554 - val_loss: 0.9748 - val_acc: 0.6519
Epoch 12/25
Epoch 00011: val_loss did not improve
8s - loss: 0.9607 - acc: 0.6594 - val_loss: 0.9750 - val_acc: 0.6439
Epoch 13/25
Epoch 00012: val_loss improved from 0.96964 to 0.94012, saving model to IN_models/cnn-adadelta-150-True-categorical_crossentropy-128-0.5-init-pretrained-90.0-128-128.model.cl.6in
8s - loss: 0.9503 - acc: 0.6629 - val_loss: 0.9401 - val_acc: 0.6643
Epoch 14/25
Epoch 00013: val_loss did not improve
8s - loss: 0.9380 - acc: 0.6660 - val_loss: 0.9739 - val_acc: 0.6652
Epoch 15/25
Epoch 00014: val_loss improved from 0.94012 to 0.92391, saving model to IN_models/cnn-adadelta-150-True-categorical_crossentropy-128-0.5-init-pretrained-90.0-128-128.model.cl.6in
8s - loss: 0.9317 - acc: 0.6657 - val_loss: 0.9239 - val_acc: 0.6687
Epoch 16/25
Epoch 00015: val_loss did not improve
8s - loss: 0.9233 - acc: 0.6657 - val_loss: 0.9310 - val_acc: 0.6723
Epoch 17/25
Epoch 00016: val_loss did not improve
8s - loss: 0.9184 - acc: 0.6734 - val_loss: 0.9301 - val_acc: 0.6740
Epoch 18/25
Epoch 00017: val_loss did not improve
8s - loss: 0.8964 - acc: 0.6762 - val_loss: 0.9267 - val_acc: 0.6687
Epoch 19/25
Epoch 00018: early stopping
Epoch 00018: val_loss did not improve
8s - loss: 0.8890 - acc: 0.6807 - val_loss: 0.9271 - val_acc: 0.6811
Test model ...
Loading ... IN_models/cnn-adadelta-150-True-categorical_crossentropy-128-0.5-init-pretrained-90.0-128-128.model.cl.6in
  32/2261 [..............................] - ETA: 0s  64/2261 [..............................] - ETA: 0s  96/2261 [>.............................] - ETA: 0s 128/2261 [>.............................] - ETA: 0s 160/2261 [=>............................] - ETA: 0s 192/2261 [=>............................] - ETA: 0s 224/2261 [=>............................] - ETA: 0s 256/2261 [==>...........................] - ETA: 0s 288/2261 [==>...........................] - ETA: 0s 320/2261 [===>..........................] - ETA: 0s 352/2261 [===>..........................] - ETA: 0s 384/2261 [====>.........................] - ETA: 0s 416/2261 [====>.........................] - ETA: 0s 448/2261 [====>.........................] - ETA: 0s 480/2261 [=====>........................] - ETA: 0s 512/2261 [=====>........................] - ETA: 0s 544/2261 [======>.......................] - ETA: 0s 576/2261 [======>.......................] - ETA: 0s 608/2261 [=======>......................] - ETA: 0s 640/2261 [=======>......................] - ETA: 0s 672/2261 [=======>......................] - ETA: 0s 704/2261 [========>.....................] - ETA: 0s 736/2261 [========>.....................] - ETA: 0s 768/2261 [=========>....................] - ETA: 0s 800/2261 [=========>....................] - ETA: 0s 832/2261 [==========>...................] - ETA: 0s 864/2261 [==========>...................] - ETA: 0s 896/2261 [==========>...................] - ETA: 0s 928/2261 [===========>..................] - ETA: 0s 960/2261 [===========>..................] - ETA: 0s 992/2261 [============>.................] - ETA: 0s1024/2261 [============>.................] - ETA: 0s1056/2261 [=============>................] - ETA: 0s1088/2261 [=============>................] - ETA: 0s1120/2261 [=============>................] - ETA: 0s1152/2261 [==============>...............] - ETA: 0s1184/2261 [==============>...............] - ETA: 0s1216/2261 [===============>..............] - ETA: 0s1248/2261 [===============>..............] - ETA: 0s1280/2261 [===============>..............] - ETA: 0s1312/2261 [================>.............] - ETA: 0s1344/2261 [================>.............] - ETA: 0s1376/2261 [=================>............] - ETA: 0s1408/2261 [=================>............] - ETA: 0s1440/2261 [==================>...........] - ETA: 0s1472/2261 [==================>...........] - ETA: 0s1504/2261 [==================>...........] - ETA: 0s1536/2261 [===================>..........] - ETA: 0s1568/2261 [===================>..........] - ETA: 0s1600/2261 [====================>.........] - ETA: 0s1632/2261 [====================>.........] - ETA: 0s1664/2261 [=====================>........] - ETA: 0s1696/2261 [=====================>........] - ETA: 0s1728/2261 [=====================>........] - ETA: 0s1760/2261 [======================>.......] - ETA: 0s1792/2261 [======================>.......] - ETA: 0s1824/2261 [=======================>......] - ETA: 0s1856/2261 [=======================>......] - ETA: 0s1888/2261 [========================>.....] - ETA: 0s1920/2261 [========================>.....] - ETA: 0s1952/2261 [========================>.....] - ETA: 0s1984/2261 [=========================>....] - ETA: 0s2016/2261 [=========================>....] - ETA: 0s2048/2261 [==========================>...] - ETA: 0s2080/2261 [==========================>...] - ETA: 0s2112/2261 [===========================>..] - ETA: 0s2144/2261 [===========================>..] - ETA: 0s2176/2261 [===========================>..] - ETA: 0s2208/2261 [============================>.] - ETA: 0s2240/2261 [============================>.] - ETA: 0s2261/2261 [==============================] - 0s     
Raw Accuracy: 0.669172932331
['Affected individuals', 'Donations and volunteering', 'Infrastructure and utilities', 'Not related or irrelevant', 'Other Useful Information', 'Sympathy and support']
[0, 1, 2, 3, 4, 5]
                              precision    recall  f1-score   support

        Affected individuals       0.52      0.53      0.53       151
  Donations and volunteering       0.57      0.63      0.60       204
Infrastructure and utilities       0.83      0.07      0.13        70
   Not related or irrelevant       0.71      0.90      0.80      1339
    Other Useful Information       0.48      0.21      0.29       301
        Sympathy and support       0.57      0.15      0.23       196

                 avg / total       0.65      0.67      0.62      2261

Confusion Matrix:
 [[  80   12    0   49    9    1]
 [   2  129    0   61    9    3]
 [  14    0    5   37   14    0]
 [  36   44    0 1207   34   18]
 [  18   33    1  186   63    0]
 [   3    8    0  155    1   29]]
 micro pre: 0.669172932331 rec: 0.669172932331 f-score: 0.669172932331
 macro pre: 0.615390473563 rec: 0.415377219291 f-score: 0.430117540987
----------------------------------------------------------------------
